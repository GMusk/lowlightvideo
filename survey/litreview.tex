\documentclass[12pt,a4paper]{article}
\usepackage{times}
\usepackage{durhampaper}
\usepackage{harvard}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{etoolbox}
\usepackage{caption}
\usepackage{subcaption}

\graphicspath{ {./images/} }

\citationmode{abbr}
\bibliographystyle{agsm}


\title{Dark Feature Enhancement [provisional]}
\author{George Musker}
\student{George Musker}
\supervisor{Dr. George-Alex Koulieris}
%\degree{MEng Computer Science}

%\date{}

\begin{document}

\maketitle

\section{Introduction}

	This paper will outline and review similar work in the field of video and image enhancement emphasizing each component of the pipeline. 

	The displays that we view content are always improving, currently there is a large interest in the use of high dynamic range (HDR) displays. These displays can express much more detail and contrast in images and videos. As well as this, there is a large focus on improving capture technology, whether it is to improve capture explicitly - namely capturing HDR content from home - or capturing superior footage in difficult conditions. Our project aims to restore contrast and detail to low light, underexposed video.



\section{Literature Review}

\subsection{Tone Mapping}

	This section outlines work using tone mapping and its inverse for increasing the dynamic range of image and video. For a more in-depth analysis, \cite{doi:10.1111/j.1467-8659.2009.01541.x} created a survey of all work before 2010. 

	The problem of expanding LDR content to HDR content is well established by \cite{Banterle:2006:ITM:1174429.1174489}. Their project introduces the problem of inverse tone-mapping for increasing the dynamic range of LDR content. The goal here is to tackle the large number of legacy videos and images that have a LDR whether this is from the nature of capture, or the equipment used to acquire it. Although the work outlined here is separate to our project it is still of note, as low light video is effectively video with low dynamic range. In the paper, they seek to increase the dynamic range of images by locating the over saturated areas of an image and expanding the colour depth at these locations.  This is done by using a median cut algorithm to locate areas of high luminance and then applying an expand map. The paper does apply this algorithm to video, but given the nature of the expand map, the algorithm is not temporally coherent resulting in flickering artefacts. However, in \cite{Rempel:2007:LOR:1276377.1276426}, a temporally coherent algorithm is designed. Based off of the design of \cite{Banterle:2006:ITM:1174429.1174489}, they use a spatial bilateral filter to reduce noise then apply a brightness enhancement function. The brightness enhancement function increases depth in saturated areas but maintains clear edge contrast by using flood fill edge stopping function. In this paper the focus is to create a real-time, automatic converter.

	\cite{Banterle:2008:ELD:1921264.1921275} address the issue of temporal coherence on LDR video with their work in this paper. Where the previous design suffered from the expand map being a frame by frame algorithm, here a temporal expand map. The median cut algorithm previously mentioned is expanded to use time as a third dimension. However, temporal density approximation is costly and therefore the solution cannot run in real-time a problem which we hope to address.

	A semi-automatic system is proposed by \cite{doi:10.1111/j.1467-8659.2008.01265.x}. Again, the paper focuses on enhancing bright regions within the image. The videos are pre-processed to detect motion flow and clipped regions. A classifier is then used on these regions to identify scene objects, which results in a system robust to pronounced noise and contouring problems. However, this system requires the manual entry of variables for tuning the system, something that an automatic system cannot have. Machine learning is also used in \cite{endo2017deep} and \cite{Eilertsen:2017:HIR:3130800.3130816} to create deep reverse tone mappers. Although our project does not wish to use a completely ML system, the use of machine learning components may be a useful way to improve computation times. 

	%\cite{7952501}

	%\cite{Oskarsson2017}

	%\cite{Temporaladaptationcontrolforlocaltonemappingoperator}

	%\cite{7532485}

	
	%\cite{Eilertsen:2015:RNT:2816795.2818092}

\subsection{Image Filters}

	% Processing images often use some form of high-dimensional guassian filtering, bilateral filtering and non-local means being two such algorithms.s
	
	Low-light video is inherently a noisy source, due to the low exposure and high camera gain.  Most frameworks will use some form of filter to smooth noisy areas, however it is paramount to not lose any of the fine detail that is hidden within the video. This section will review methods for improving existing image filters.

	A large number of papers have made use of some form of the bilateral filter. The bilateral filter uses a Guassian kernel to smooth the source image while preserving edges by accounting for difference in colour intensity and range differences. A number of papers have implemented more computationally efficient bilateral filters, which are especially relevant if the a low-light video is to be adjusted in real-time. \cite{adams2010fast} propose a method which "which tessellates high-dimensional space with uniform simplices". This reduces both the space and time complexities of such algorithms. Similarly, \cite{Adams:2009:GKF:1531326.1531327} propose a method of accelerating filters like the bilateral. Makes use of Monte-Carlo kd tree sampling algorithm. The algorithm takes a significant amount of time to build the tree however, a problem for real-time solutions.
	\cite{frosio2015machine} uses machine learning to create a bilateral filter, this filter outperforms other filters that require ruled-based tuning.

	%	\cite{Szczepanski2019}

\subsection{Low Light video}

	This problem was tackled in \cite{Bennett:2005:VEU:1073204.1073272}, where each pixel's exposure is adaptively varied, using a spatiotemporal filter. This filter is integrated temporally, expanding the dynamic range of the frame while reducing noise. The result is a temporally coherent video. Our project hopes to address real time low-light enhancement, a problem that isn't addressed in this paper. Similarly, the problem of enhancing low light video low light video is also done by \cite{malm2007adaptive}. The algorithm shares a similar design with \cite{Bennett:2005:VEU:1073204.1073272}, but rather than tone mapping with a logarithmic function they instead use a contrast limited histogram equalisation function. Their results showed a better noise reduction than (Bennett) but there was a clear loss of contrast, a shortcoming resultant of an automatic algorithm. The paper does not produce a real-time solution.

	Work was also done on deblurring noisy/blurred pairs of images by \cite{Yuan:2007:IDB:1276377.1276379}. Given long exposure leads to shaky captures and low exposure leads to dark and noisy captures, they seeked to combine each image into high qualtiy image. The model required a spatially invariant kernel and two images, a method inappropriate for automatic video enhancement with motion, however should be of note given the overall effort is to regain detail my integrating over similar frames - much like videos.

	Real time work continued by \cite{Kovaleski2009}. The algorithm uses bilateral filtering, improved by the use of a bilateral grid for faster computation. The result is fully automatic and is implemented effectively onto GPUs.

	\cite{6460559} propose an algorithm which uses luminance maps to acquire detail from low-light frames. The images are then de-hazed by inverting the luminance map, producing a higher contrast image albeit with noise. This noise is addressed by applying a joint-bilateral filter using the green channel to supply edge detail. This method would not work on videos as temporal coherence is not adddressed.

	\cite{li2015low} use a superpixel method to split image into patches. The paper found that the change in gradients at superpixels was large for noisy regions, assuming the noise is additive white Guassian noise, and the change was low in textured regions. Using this, the de-noising filter can be adaptively applied. Path based denoising is also done by \cite{kim2015novel}. Here, they reduce termporal noise - the knaive solution would be to average along temporal dimension but this would leave ghosting artefacts on regions containing motion. Again, a spatio-temporal filter is used. The result is then tone-mapped with a clipping function using gamma correction. Uses non local means to handle remaining spatial noise.

	
	%\cite{Lian2017} // dehazing

	%\cite{ali2018denoising}





 
	%\cite{doi:10.1111/j.1467-8659.2009.01541.x} this is a survey paper.











\bibliography{litreview}


\end{document}